{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db03b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Inference script for Optimized Hierarchical ClinicalBERT\n",
    "========================================================\n",
    "Returns disease names instead of numeric labels\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from optimized_clinical_bert import (\n",
    "    ModelConfig,\n",
    "    OptimizedHierarchicalClinicalBERT\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Label mappings (IMPORTANT)\n",
    "# -----------------------------\n",
    "PARENT_ID2LABEL = {\n",
    "    0: \"general pathological conditions\",\n",
    "    1: \"specific disease\"\n",
    "}\n",
    "\n",
    "CHILD_ID2LABEL = {\n",
    "    0: \"neoplasms\",\n",
    "    1: \"digestive system diseases\",\n",
    "    2: \"nervous system diseases\",\n",
    "    3: \"cardiovascular diseases\"\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load Model\n",
    "# -----------------------------\n",
    "def load_model(\n",
    "    model_path: str,\n",
    "    model_name: str = \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    device: str = None\n",
    "):\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    config = ModelConfig(\n",
    "        model_name=model_name,\n",
    "        use_lora=True,\n",
    "        use_gradient_checkpointing=False  # disable for inference\n",
    "    )\n",
    "\n",
    "    model = OptimizedHierarchicalClinicalBERT(config)\n",
    "    state_dict = load_file(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Single Text Inference\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_single(\n",
    "    text: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_len: int = 256\n",
    ") -> Dict:\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "\n",
    "    # Full softmax probabilities\n",
    "    parent_probs = torch.softmax(outputs[\"parent_logits\"], dim=-1).squeeze(0)  # (2,)\n",
    "    child_probs = torch.softmax(outputs[\"child_logits\"], dim=-1).squeeze(0)    # (4,)\n",
    "\n",
    "    parent_pred = parent_probs.argmax().item()\n",
    "    child_pred = child_probs.argmax().item()\n",
    "    parent_conf = parent_probs.max().item()\n",
    "\n",
    "\n",
    "    # Hierarchical decoding\n",
    "    if parent_conf >= 0.8:  # general\n",
    "        predicted_disease = PARENT_ID2LABEL[0]\n",
    "        child_label = None\n",
    "    else:\n",
    "        predicted_disease = CHILD_ID2LABEL[child_pred]\n",
    "        child_label = CHILD_ID2LABEL[child_pred]\n",
    "\n",
    "    # Return ALL probabilities as dicts\n",
    "    parent_probs_dict = {label: float(prob) for label, prob in zip(PARENT_ID2LABEL.values(), parent_probs.tolist())}\n",
    "    child_probs_dict = {label: float(prob) for label, prob in zip(CHILD_ID2LABEL.values(), child_probs.tolist())}\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"predicted_disease\": predicted_disease,\n",
    "        \"parent_label\": PARENT_ID2LABEL[parent_pred],\n",
    "        \"parent_probs\": parent_probs_dict,\n",
    "        \"child_label\": child_label,\n",
    "        \"child_probs\": child_probs_dict\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Batch Inference\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_batch(\n",
    "    texts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    batch_size: int = 8,\n",
    "    max_len: int = 256\n",
    ") -> List[Dict]:\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        parent_probs = torch.softmax(outputs[\"parent_logits\"], dim=-1)\n",
    "        child_probs = torch.softmax(outputs[\"child_logits\"], dim=-1)\n",
    "\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            parent_pred = parent_probs[j].argmax().item()\n",
    "            child_pred = child_probs[j].argmax().item()\n",
    "\n",
    "            parent_conf = parent_probs[j].max().item()\n",
    "            child_conf = child_probs[j].max().item()\n",
    "\n",
    "            # -----------------------------\n",
    "            # SAME hierarchical logic as single inference\n",
    "            # -----------------------------\n",
    "            if parent_conf > 0.8:\n",
    "                predicted_disease = PARENT_ID2LABEL[0]\n",
    "                child_label = None\n",
    "                child_confidence = None\n",
    "            else:\n",
    "                predicted_disease = CHILD_ID2LABEL[child_pred]\n",
    "                child_label = CHILD_ID2LABEL[child_pred]\n",
    "                child_confidence = child_conf\n",
    "\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"predicted_disease\": predicted_disease,\n",
    "                \"parent_label\": PARENT_ID2LABEL[parent_pred],\n",
    "                \"parent_confidence\": parent_conf,\n",
    "                \"child_label\": child_label,\n",
    "                \"child_confidence\": child_confidence\n",
    "            })\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Run inference on a test CSV file\n",
    "================================\n",
    "Input: condition_label, medical_abstract\n",
    "Output: predictions with disease names\n",
    "\"\"\"\n",
    "GT_ID2LABEL = {\n",
    "    1: \"neoplasms\",\n",
    "    2: \"digestive system diseases\",\n",
    "    3: \"nervous system diseases\",\n",
    "    4: \"cardiovascular diseases\",\n",
    "    5: \"general pathological conditions\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Run inference on CSV\n",
    "# -----------------------------\n",
    "def run_inference_on_csv(\n",
    "    csv_path: str =\"./dataset/test.csv\",\n",
    "    model_path: str = \"./results/best_model.safetensors\",\n",
    "    output_path: str = \"./dataset/prediction.csv\"\n",
    "):\n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    assert \"medical_abstract\" in df.columns, \"CSV must contain medical_abstract\"\n",
    "    assert \"condition_label\" in df.columns, \"CSV must contain condition_label\"\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer, device = load_model(model_path)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"medical_abstract\"]\n",
    "        true_label = int(row[\"condition_label\"])\n",
    "\n",
    "        result = predict_single(\n",
    "            text=text,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        predictions.append({\n",
    "            \"medical_abstract\": text,\n",
    "            \"true_label_id\": true_label,\n",
    "            \"true_disease\": GT_ID2LABEL[true_label],\n",
    "            \"predicted_disease\": result[\"predicted_disease\"],\n",
    "            \"parent_label\": result[\"parent_label\"],\n",
    "            \"parent_confidence\": result[\"parent_confidence\"],\n",
    "            \"child_label\": result[\"child_label\"],\n",
    "            \"child_confidence\": result[\"child_confidence\"]\n",
    "        })\n",
    "\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Inference complete. Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d40ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference_on_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "103e41c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (6.3.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (6.5.1)\n",
      "Collecting captum\n",
      "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (4.12.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.128.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.3 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (2.4.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (3.11.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (2.12.5)\n",
      "Requirement already satisfied: pydub in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.0.21)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio) (0.40.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from gradio-client==2.0.3->gradio) (2025.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from plotly) (2.15.0)\n",
      "Collecting matplotlib (from captum)\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-win_amd64.whl.metadata (52 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: torch>=1.10 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from captum) (2.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from torch>=1.10->captum) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from torch>=1.10->captum) (3.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from sympy>=1.13.3->torch>=1.10->captum) (1.3.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->captum)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->captum)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->captum)\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->captum)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->captum)\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.6.3)\n",
      "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 18.1 MB/s  0:00:00\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Downloading matplotlib-3.10.8-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 6.6/8.1 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 33.6 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 33.4 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "Using cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib, captum\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.4.1\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "    Uninstalling numpy-2.4.1:\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "      Successfully uninstalled numpy-2.4.1\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   --------------- ------------------------ 3/8 [fonttools]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ------------------------------ --------- 6/8 [matplotlib]\n",
      "   ----------------------------------- ---- 7/8 [captum]\n",
      "   ---------------------------------------- 8/8 [captum]\n",
      "\n",
      "Successfully installed captum-0.8.0 contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 numpy-1.26.4 pyparsing-3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\fmahnoor1\\AppData\\Local\\Temp\\pip-uninstall-e9awvkbg'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio plotly captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea78bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://fd0f43955e3391ec39.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://fd0f43955e3391ec39.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load model once\n",
    "# -----------------------------\n",
    "MODEL_PATH = \"./results/best_model.safetensors\"\n",
    "model, tokenizer, device = load_model(MODEL_PATH)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Function to create bar plots\n",
    "# -----------------------------\n",
    "def plot_probs(prob_dict, title=\"Probabilities\"):\n",
    "    labels = list(prob_dict.keys())\n",
    "    values = [v if v is not None else 0 for v in prob_dict.values()]\n",
    "    \n",
    "    fig = go.Figure([go.Bar(x=labels, y=values, text=[round(v,3) for v in values], textposition='auto')])\n",
    "    fig.update_layout(title_text=title, yaxis=dict(range=[0,1]), margin=dict(t=40, b=20))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction function for Gradio\n",
    "# -----------------------------\n",
    "def classify_text(text: str):\n",
    "    result = predict_single(text, model, tokenizer, device)\n",
    "\n",
    "    # Parent and child probability dicts\n",
    "    parent_display = {label: round(prob,3) for label, prob in result[\"parent_probs\"].items()}\n",
    "\n",
    "    if result[\"child_label\"] is None:\n",
    "        child_display = {label: 0 for label in CHILD_ID2LABEL.values()}\n",
    "    else:\n",
    "        child_display = {label: round(prob,3) for label, prob in result[\"child_probs\"].items()}\n",
    "\n",
    "    # Build plots\n",
    "    parent_plot = plot_probs(parent_display, title=\"Parent Probabilities\")\n",
    "    child_plot = plot_probs(child_display, title=\"Child Probabilities\")\n",
    "\n",
    "    return result[\"predicted_disease\"], parent_plot, child_plot\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio Interface\n",
    "# -----------------------------\n",
    "iface = gr.Interface(\n",
    "    fn=classify_text,\n",
    "    inputs=gr.Textbox(lines=1, placeholder=\"Enter medical abstract here...\"),\n",
    "    outputs=[\n",
    "        gr.Label(label=\"Predicted Disease\"),\n",
    "        gr.Plot(label=\"Parent Probabilities\"),\n",
    "        gr.Plot(label=\"Child Probabilities\")\n",
    "    ],\n",
    "    title=\"Hierarchical ClinicalBERT Disease Classifier\",\n",
    "    description=\"Enter a medical abstract to get predicted disease and probability plots for parent and child disease categories.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3336979",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Integrated Gradients\u001b[39;00m\n\u001b[32m     25\u001b[39m ig = IntegratedGradients(forward_parent)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m attributions, delta = \u001b[43mig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Decode tokens\u001b[39;00m\n\u001b[32m     29\u001b[39m tokens = tokenizer.convert_ids_to_tokens(input_ids[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:289\u001b[39m, in \u001b[36mIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[39m\n\u001b[32m    277\u001b[39m     attributions = _batch_attribution(\n\u001b[32m    278\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m         num_examples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m         method=method,\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     attributions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_baselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[32m    299\u001b[39m     start_point, end_point = baselines, inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:368\u001b[39m, in \u001b[36mIntegratedGradients._attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[39m\n\u001b[32m    365\u001b[39m expanded_target = _expand_target(target, n_steps)\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[32m    377\u001b[39m scaled_grads = [\n\u001b[32m    378\u001b[39m     grad.contiguous().view(n_steps, -\u001b[32m1\u001b[39m)\n\u001b[32m    379\u001b[39m     * torch.tensor(step_sizes).float().view(n_steps, \u001b[32m1\u001b[39m).to(grad.device)\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[32m    381\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\_utils\\gradient.py:128\u001b[39m, in \u001b[36mcompute_gradients\u001b[39m\u001b[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03marbitrary forward function.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    124\u001b[39m \u001b[33;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     outputs = \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[32m    130\u001b[39m     \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[32m    132\u001b[39m     outputs = cast(Tensor, outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\_utils\\common.py:588\u001b[39m, in \u001b[36m_run_forward\u001b[39m\u001b[34m(forward_func, inputs, target, additional_forward_args)\u001b[39m\n\u001b[32m    585\u001b[39m inputs = _format_inputs(inputs)\n\u001b[32m    586\u001b[39m additional_forward_args = _format_additional_forward_args(additional_forward_args)\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m output = \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001b[39;49;00m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    594\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, torch.futures.Future):\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output.then(\u001b[38;5;28;01mlambda\u001b[39;00m x: _select_targets(x.value(), target))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mforward_parent\u001b[39m\u001b[34m(input_ids)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_parent\u001b[39m(input_ids):\n\u001b[32m     17\u001b[39m     attention_mask = (input_ids != tokenizer.pad_token_id).long()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Return logits for the predicted parent class\u001b[39;00m\n\u001b[32m     20\u001b[39m     parent_logits = outputs[\u001b[33m\"\u001b[39m\u001b[33mparent_logits\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\Documents\\Fall25\\PersonalProjects\\HierarchicalClinicalBERT\\optimized_clinical_bert.py:235\u001b[39m, in \u001b[36mOptimizedHierarchicalClinicalBERT.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, return_embeddings)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    222\u001b[39m     input_ids: torch.Tensor,\n\u001b[32m    223\u001b[39m     attention_mask: torch.Tensor,\n\u001b[32m    224\u001b[39m     return_embeddings: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    225\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m        input_ids: (batch_size, seq_len)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m \u001b[33;03m        dict with 'parent_logits', 'child_logits', and optionally 'embeddings'\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     parent_logits = \u001b[38;5;28mself\u001b[39m.parent_head(embeddings)\n\u001b[32m    237\u001b[39m     child_logits = \u001b[38;5;28mself\u001b[39m.child_head(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\Documents\\Fall25\\PersonalProjects\\HierarchicalClinicalBERT\\optimized_clinical_bert.py:137\u001b[39m, in \u001b[36mEfficientClinicalBERTEncoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        input_ids: (batch_size, seq_len)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        pooled: (batch_size, hidden_size)\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# Use pooled output or mean pooling\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[33m'\u001b[39m\u001b[33mpooler_output\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m outputs.pooler_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:936\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    945\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:179\u001b[39m, in \u001b[36mBertEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    176\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=\u001b[38;5;28mself\u001b[39m.position_ids.device)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m token_type_embeddings = \u001b[38;5;28mself\u001b[39m.token_type_embeddings(token_type_ids)\n\u001b[32m    182\u001b[39m embeddings = inputs_embeds + token_type_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from captum.attr import IntegratedGradients\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use your model and tokenizer\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tokenizer  # already loaded\n",
    "\n",
    "# Encode text\n",
    "text = \"Noninvasive determination of pulmonary artery wedge pressure: comparative analysis of pulsed Doppler echocardiography and right heart catheterization. To compare left ventricular filling variables as derived by transmitral pulsed Doppler echocardiography (tpDE) and hemodynamic variables as assessed at right heart catheterization (RHC), 104 ICU patients (64 male, 40 female) aged 26 to 73 yr (mean 54.6 +/- 10.3) without valvular heart disease were examined. Simultaneously with RHC, transmitral flow velocity profiles were obtained by tpDE, and the ratio of the velocity-time integrals of late diastolic active (A wave) and early diastolic passive inflow into the left ventricle (E wave) was calculated (A/E ratio). Invasively determined pulmonary capillary wedge pressure (WP) ranged from 3 to 36 mm Hg (median 13.35, 5%/95% 6/31 mm Hg). Linear regression analysis showed a highly significant correlation between the A/E ratio and WP (r = .98, p less than .001, standard error of the estimate [SEE] = 0.10). The A/E ratio also correlated with other hemodynamic variables such as cardiac output (r = -.68, p less than .001, SEE = 0.33), cardiac index (r = -.74, p less than .001, SEE = 0.31), and stroke volume index (r = -.68, p less than .001, SEE = 0.34). The interobserver agreement (derived by intraclass correlation analysis between two examiners) on the A/E ratio was high (r = .95, p less than .001, n = 26). We conclude that WP can be accurately determined noninvasively by tpDE. For the assessment of systolic ventricular function, tpDE is of limited diagnostic value.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Define a wrapper for Captum\n",
    "def forward_parent(input_ids):\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    # Return logits for the predicted parent class\n",
    "    parent_logits = outputs[\"parent_logits\"]\n",
    "    parent_class = parent_logits.argmax(dim=-1)\n",
    "    return parent_logits[:, parent_class]\n",
    "\n",
    "# Integrated Gradients\n",
    "ig = IntegratedGradients(forward_parent)\n",
    "attributions, delta = ig.attribute(input_ids, return_convergence_delta=True)\n",
    "\n",
    "# Decode tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "# Map token -> importance\n",
    "word_importance = {tok: float(attr) for tok, attr in zip(tokens, attributions[0])}\n",
    "\n",
    "# Sort by absolute importance\n",
    "sorted_importance = dict(sorted(word_importance.items(), key=lambda x: abs(x[1]), reverse=True))\n",
    "\n",
    "print(sorted_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8076a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (0.50.0)\n",
      "Requirement already satisfied: numpy==2.3 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (1.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (2.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (0.63.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from numba>=0.54->shap) (0.46.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from pandas->shap) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from scikit-learn->shap) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\fmahnoor1\\appdata\\local\\anaconda3\\envs\\medical_classifier_gpu\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OptimizedHierarchicalClinicalBERT.forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m model, tokenizer, device = load_model(MODEL_PATH)\n\u001b[32m      6\u001b[39m explainer = shap.Explainer(model, tokenizer)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m shap_values = \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPatient presents with chest pain and shortness of breath.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m shap.plots.text(shap_values[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\shap\\explainers\\_partition.py:173\u001b[39m, in \u001b[36mPartitionExplainer.__call__\u001b[39m\u001b[34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    163\u001b[39m     *args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     silent=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    171\u001b[39m ):\n\u001b[32m    172\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfixed_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\shap\\explainers\\_explainer.py:364\u001b[39m, in \u001b[36mExplainer.__call__\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m     feature_names = [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(*args), num_rows, \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m explainer\u001b[39m\u001b[33m\"\u001b[39m, silent):\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     row_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     values.append(row_result.get(\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    375\u001b[39m     output_indices.append(row_result.get(\u001b[33m\"\u001b[39m\u001b[33moutput_indices\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\shap\\explainers\\_partition.py:204\u001b[39m, in \u001b[36mPartitionExplainer.explain_row\u001b[39m\u001b[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# if not fixed background or no base value assigned then compute base value for a row\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._curr_base_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.masker, \u001b[33m\"\u001b[39m\u001b[33mfixed_background\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28mself\u001b[39m._curr_base_value = \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm00\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[32m    205\u001b[39m         \u001b[32m0\u001b[39m\n\u001b[32m    206\u001b[39m     ]  \u001b[38;5;66;03m# the zero index param tells the masked model what the baseline is\u001b[39;00m\n\u001b[32m    207\u001b[39m f11 = fm(~m00.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m))[\u001b[32m0\u001b[39m]\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.masker.clustering):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\shap\\utils\\_masked_model.py:67\u001b[39m, in \u001b[36mMaskedModel.__call__\u001b[39m\u001b[34m(self, masks, zero_index, batch_size)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._full_masking_call(full_masks, zero_index=zero_index, batch_size=batch_size)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\shap\\utils\\_masked_model.py:142\u001b[39m, in \u001b[36mMaskedModel._full_masking_call\u001b[39m\u001b[34m(self, masks, zero_index, batch_size)\u001b[39m\n\u001b[32m    139\u001b[39m         all_masked_inputs[i].append(v)\n\u001b[32m    141\u001b[39m joined_masked_inputs = \u001b[38;5;28mtuple\u001b[39m([np.concatenate(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_masked_inputs])\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[32m    144\u001b[39m all_outputs.append(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\shap\\models\\_model.py:23\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     is_tensor = safe_isinstance(out, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     out = out.cpu().detach().numpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np.array(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: OptimizedHierarchicalClinicalBERT.forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "!pip install shap numpy==2.3\n",
    "import shap\n",
    "\n",
    "MODEL_PATH = \"./results/best_model.safetensors\"\n",
    "model, tokenizer, device = load_model(MODEL_PATH)\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer([\"Patient presents with chest pain and shortness of breath.\"])\n",
    "\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cd2aeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Target not provided when necessary, cannot take gradient with respect to multiple outputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Integrated Gradients\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     32\u001b[39m ig = IntegratedGradients(forward_parent)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m attributions, delta = \u001b[43mig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m tokens = tokenizer.convert_ids_to_tokens(input_ids[\u001b[32m0\u001b[39m])\n\u001b[32m     37\u001b[39m word_importance = {tok: \u001b[38;5;28mfloat\u001b[39m(attr) \u001b[38;5;28;01mfor\u001b[39;00m tok, attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tokens, attributions[\u001b[32m0\u001b[39m])}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[39m, in \u001b[36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: Any, **kwargs: Any):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:289\u001b[39m, in \u001b[36mIntegratedGradients.attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[39m\n\u001b[32m    277\u001b[39m     attributions = _batch_attribution(\n\u001b[32m    278\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    279\u001b[39m         num_examples,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m         method=method,\n\u001b[32m    287\u001b[39m     )\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     attributions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatted_baselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[32m    299\u001b[39m     start_point, end_point = baselines, inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:368\u001b[39m, in \u001b[36mIntegratedGradients._attribute\u001b[39m\u001b[34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[39m\n\u001b[32m    365\u001b[39m expanded_target = _expand_target(target, n_steps)\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m grads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[32m    377\u001b[39m scaled_grads = [\n\u001b[32m    378\u001b[39m     grad.contiguous().view(n_steps, -\u001b[32m1\u001b[39m)\n\u001b[32m    379\u001b[39m     * torch.tensor(step_sizes).float().view(n_steps, \u001b[32m1\u001b[39m).to(grad.device)\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[32m    381\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fmahnoor1\\AppData\\Local\\anaconda3\\envs\\medical_classifier_gpu\\Lib\\site-packages\\captum\\_utils\\gradient.py:133\u001b[39m, in \u001b[36mcompute_gradients\u001b[39m\u001b[34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[32m    132\u001b[39m outputs = cast(Tensor, outputs)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[32m0\u001b[39m].numel() == \u001b[32m1\u001b[39m, (\n\u001b[32m    134\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTarget not provided when necessary, cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m take gradient with respect to multiple outputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n\u001b[32m    139\u001b[39m grads = torch.autograd.grad(torch.unbind(outputs), inputs)\n",
      "\u001b[31mAssertionError\u001b[39m: Target not provided when necessary, cannot take gradient with respect to multiple outputs."
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "from captum.attr import IntegratedGradients\n",
    "import torch\n",
    "\n",
    "model, tokenizer, device = load_model(\n",
    "    model_path=\"./results/best_model.safetensors\"\n",
    ")\n",
    "\n",
    "text = \"Patient presents with chest pain and shortness of breath.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Wrapper for Captum\n",
    "# -----------------------------\n",
    "def forward_parent(input_ids):\n",
    "    # Ensure correct dtype\n",
    "    input_ids = input_ids.long()\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    \n",
    "    # Get parent logits for the predicted class\n",
    "    parent_logits = outputs[\"parent_logits\"]\n",
    "    parent_class = parent_logits.argmax(dim=-1)\n",
    "    return parent_logits[:, parent_class]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Integrated Gradients\n",
    "# -----------------------------\n",
    "ig = IntegratedGradients(forward_parent)\n",
    "\n",
    "attributions, delta = ig.attribute(input_ids, return_convergence_delta=True)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "word_importance = {tok: float(attr) for tok, attr in zip(tokens, attributions[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60da90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45be1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_classifier_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
